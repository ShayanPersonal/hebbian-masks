# hebbian

This is a experiment with what I call "hebbian masks". The idea is to prune unused weights with the hebbian learning rule.

When applied to fully connected layers it seems to have no affect on learning but significantly increases the sparsity of connections.
